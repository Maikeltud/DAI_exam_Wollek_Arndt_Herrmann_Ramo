{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ss1fcoFeJhpG"
   },
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9u-Wi7oJhpG"
   },
   "source": [
    "## 1.1 Quick Summary\n",
    "### Insider trading\n",
    "Insider trading involves the buying or selling of a public company's stock by individuals who have access to non-public, material information about the company. Legal insider trading is regulated and must be disclosed publicly, while illegal insider trading involves trading based on confidential information that gives an unfair advantage. The distinction between legal and illegal insider trading is defined under Section 16 of the Securities Exchange Act, which mandates insiders to publicly disclose their trades.\n",
    "### Why should insider trading play a role when making investment decisions?\n",
    "Insider trading can provide valuable insights into the future prospects of a company. When insiders, such as executives and directors, buy or sell shares, it often signals their confidence or concerns about the company's performance. Tracking these trades can help investors make more informed decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ft2yKJTwJhpH"
   },
   "source": [
    "## 1.2 Research Goals\n",
    "### Which insights does insider trading give us in terms of stock behavior?\n",
    "Insider trading activity can indicate potential stock price movements. For example, significant insider purchases may suggest that the insiders believe the stock is undervalued and poised for growth, while large sales might indicate potential issues or overvaluation. Analyzing historical insider transactions, such as those foreshadowing the tech sell-off in 2022, can reveal patterns that might predict future market behaviors.\n",
    "### How does the methodological knowledge of the lecture help us in getting to these insights?\n",
    "The methodological knowledge from the lecture, such as statistical analysis, machine learning, and financial modeling, allows us to systematically analyze insider trading patterns. By applying these techniques, we can identify trends, correlations, and anomalies in insider trading data that can be leveraged for predictive insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7evCMiEJhpH"
   },
   "source": [
    "## 1.3 Data Sources\n",
    "### What is the SEC API?\n",
    "The SEC API (Securities and Exchange Commission Application Programming Interface) provides access to a wealth of financial data, including insider trading reports. It allows users to retrieve information about insider transactions, company filings, and other regulatory disclosures in a structured and automated manner. The SEC API facilitates the extraction of data from the SEC EDGAR (Electronic Data Gathering, Analysis, and Retrieval) database, enabling detailed analysis of insider trading activities.\n",
    "### Overview of our data source\n",
    "Our primary data source is the SEC EDGAR database, accessed through the SEC API. This database contains detailed records of insider trading activities reported on Forms 3, 4, and 5.\n",
    "\n",
    "**Form 3**: This form is filed when an individual becomes an insider, such as when they are appointed as a director or officer of the company or when they acquire a significant ownership stake (typically more than 10%). It provides an initial statement of ownership.\n",
    "\n",
    "**Form 4**: This form must be filed whenever there is a change in an insider's ownership position. It discloses transactions such as purchases, sales, and transfers of the company's stock. Form 4 must be filed within two business days following the transaction, ensuring timely disclosure.\n",
    "\n",
    "**Form 5**: This form is an annual summary of insider transactions that were not reported earlier on Forms 3 or 4. It includes any previously unreported transactions from the past fiscal year and is due within 45 days after the end of the company's fiscal year.\n",
    "### Regulatory Mechanisms to Exclude Signals in Insider Trades\n",
    "Section 16(b) of the Securities Exchange Act, also known as the \"short-swing profit rule,\" plays a critical role in regulating insider trading. This rule is designed to prevent insiders—such as directors, officers, and significant shareholders (owning more than 10% of a company's shares)—from profiting from short-term trades. Specifically, if an insider buys and sells, or sells and buys, company stock within a six-month period, any profit made must be returned to the company. This regulation aims to discourage insiders from using their privileged information for quick financial gains.\n",
    "\n",
    "To address the challenge of insiders having constant access to material nonpublic information, Section 10b5-1 of the Securities Exchange Act provides a legal framework. Insiders can adopt a written trading plan specifying the amount, price, and dates of future trades. If the plan is established before the insider becomes aware of any material nonpublic information and trades are executed according to the plan, the insider is protected from allegations of insider trading.\n",
    "These plans must be disclosed, often through voluntary filings under Item 8.01 of Form 8-K. Although not mandatory, this disclosure adds another layer of transparency.\n",
    "\n",
    "Rule 10b5-1 is crucial because it aims to prevent insiders from exploiting nonpublic information for personal gain while allowing them to trade their shares lawfully. However, studies have shown that some insiders can still time the market advantageously within the boundaries of these plans, indicating that insider transactions may still carry predictive signals regarding future stock performance. For example, Elon Musk's sales of Tesla shares in late 2021 coincided with the peak of Tesla's share price, suggesting that even regulated insider transactions can provide valuable market signals. Similarly, Berkshire Hathaway's strategic purchases of Occidental Petroleum (OXY) shares in 2022, which were timed during market dips, highlight how well-timed insider trades can still offer insights into the company’s future performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HbVQ6ueUJhpH"
   },
   "source": [
    "# 2. Data Collection and Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sec_api import InsiderTradingApi\n",
    "import pandas as pd\n",
    "from pydash import get, flatten\n",
    "import warnings\n",
    "import json\n",
    "import math\n",
    "from datetime import datetime\n",
    "import ast\n",
    "import os\n",
    "import scipy.stats as stats\n",
    "from datetime import timedelta\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report, mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.utils._testing import set_random_state\n",
    "from concurrent.futures import ThreadPoolExecutor, TimeoutError\n",
    "import logging\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Frb7g4RNJhpI"
   },
   "source": [
    "## 2.2 Data Extraction\n",
    "Insider data has been created with the help of the script called \"get_insiderdata_secapi.py\" created by us. The script takes more than 15 hours to complete, thats why we didn't include it in our Jupyter Notebook.\n",
    "\n",
    "THe result is a dataset of with a total of more than 4,000,000 rows, which we will use for our analysis. We narrowed the timeframe down to 01-01-2012 to 31-12-2023 because the SEC API doesn't allow for much data before that date.\n",
    "\n",
    "The data was requested in batches of 50 (highest batch amount per request) and after that flattened and put into a csv file. We print the first five entries to ensure our data was loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "MGo2HeCnJhpJ",
    "outputId": "b6edd94a-03a1-4b39-8c06-7300e1e7aca8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"insider_trades.csv\", low_memory=False)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We exclude the columns reportingPersonName and reportingPersonCik from our analysis since they do not contribute significantly to our insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "2MRLQFp9JhpK",
    "outputId": "586f58f2-14fe-42e7-8a15-47d329415c33"
   },
   "outputs": [],
   "source": [
    "df_len_0 = len(df)\n",
    "\n",
    "print(f\"Total number of entries before mapping stock prices: {df_len_0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZdI-3pfJhpK"
   },
   "source": [
    "### Get the stock market data\n",
    "We retrieve the closing prices for each transaction using Algoseek to enrich our dataset with historical stock prices. The process involves defining a function to fetch the stock prices, iterating through our DataFrame to get the required prices, and updating the DataFrame accordingly. The prices from Algoseek have been retrieved by the department. We have mapped the prices for each stock and the according date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code will be skipped if the file \"processed_stock_data.csv\" exists. If not, remapping will occur (which will take about 24h) and the file will be recreated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "ST7AAQX3JhpK",
    "outputId": "6bebc889-3049-443c-c80f-2cc0605a5a74",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_stock_price(ticker, date, days_after=0):\n",
    "    if pd.isna(date) or date is None or date.year < 1900:\n",
    "        return 0\n",
    "    \n",
    "    try:\n",
    "        with open(f\"/home/jovyan/work/shared-for-all/algoseek_gk/algoseek/{ticker}/_closing-prices.json\", 'r') as file:\n",
    "            data = json.load(file)\n",
    "        df = pd.DataFrame.from_dict(data, orient='index')\n",
    "        df.index = pd.to_datetime(df.index.str.split('-').str[1], format='%Y%m%d')\n",
    "        df = df.sort_index()\n",
    "        \n",
    "        target_date = date + timedelta(days=days_after)\n",
    "        closest_date = df.index[df.index <= target_date].max()\n",
    "        \n",
    "        if pd.isna(closest_date):\n",
    "            return 0\n",
    "        \n",
    "        return float(df.loc[closest_date, 'adjClosePrice'])\n",
    "    except FileNotFoundError:\n",
    "        return 0\n",
    "    except Exception as e:\n",
    "        return 0\n",
    "\n",
    "def safe_to_datetime(date_str):\n",
    "    try:\n",
    "        dt = pd.to_datetime(date_str)\n",
    "        if dt.year < 1900:\n",
    "            return pd.NaT\n",
    "        return dt\n",
    "    except:\n",
    "        return pd.NaT\n",
    "\n",
    "def process_data(df):\n",
    "    total_entries = len(df)\n",
    "    print(f\"Total entries to process: {total_entries}\")\n",
    "\n",
    "    for days in [0, 1, 7, 30, 90, 360]:\n",
    "        column_name = f\"sharePrice{days}\"\n",
    "        \n",
    "        def process_row(row, index):\n",
    "            if index % 5000 == 0:\n",
    "                print(f\"Processed {index} out of {total_entries} entries for {column_name}\")\n",
    "            return get_stock_price(\n",
    "                row['issuerTicker'],\n",
    "                safe_to_datetime(row['periodOfReport']),\n",
    "                days\n",
    "            )\n",
    "        \n",
    "        df[column_name] = df.apply(\n",
    "            lambda row: process_row(row, row.name),\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        print(f\"Completed processing for {column_name}\")\n",
    "\n",
    "    print(\"All processing completed\")\n",
    "    return df\n",
    "\n",
    "# File path for the CSV\n",
    "csv_file_path = 'processed_stock_data.csv'\n",
    "\n",
    "# Check if the CSV file already exists\n",
    "if os.path.exists(csv_file_path):\n",
    "    print(f\"Processed data found at {csv_file_path}. Loading data from CSV...\")\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    print(\"Data loaded successfully.\")\n",
    "else:\n",
    "    print(\"No processed data found. Starting data processing...\")\n",
    "    df = process_data(df)\n",
    "    \n",
    "    # Save the processed data to CSV\n",
    "    df.to_csv(csv_file_path, index=False)\n",
    "    print(f\"Processed data saved to {csv_file_path}\")\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIwRpHnLJhpK"
   },
   "source": [
    "## 2.3 Data Cleaning\n",
    "Filtering our data is a crucial step in the data preparation and analysis process for several reasons:\n",
    "\n",
    "We do not want to use some kind of data which has faulty or missing values. Also we might need to transform some data in order to make it more useful. One example of this could be to transform features to only have \"true\" or \"false\" as a value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GmeK4KGYJhpK"
   },
   "source": [
    "### Remove missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove all rows where the share price at day 0, 1 day after, 7 days after, 30 days after, 90 days after, and 360 days after is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HC1vmAZoJhpK",
    "outputId": "2bba4d65-13cd-4724-db2a-5686bb9d6788"
   },
   "outputs": [],
   "source": [
    "df = df[(df[\"sharePrice0\"] != 0) | (df[\"sharePrice1\"] != 0) | (df[\"sharePrice7\"] != 0) | (df[\"sharePrice30\"] != 0) | (df[\"sharePrice90\"] != 0) | (df[\"sharePrice360\"] != 0)]\n",
    "\n",
    "df_len_1 = len(df)\n",
    "\n",
    "print(f\"Total number of entries after adjusting for no price given: {df_len_1} ({(df_len_1 - df_len_0) / df_len_1})% of rows removed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIq3fZTEJhpL"
   },
   "source": [
    "## 2.4 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new features\n",
    "We calculate the percentage of the total amount of stocks that have been sold or bought by the insider. Following this, we remove any unnecessary entries from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of the delta of the position in percent\n",
    "df[\"positionDelta\"] = (df[\"sharesOwnedFollowingTransaction\"] - df[\"shares\"]) / df[\"sharesOwnedFollowingTransaction\"] * 100\n",
    "\n",
    "# drop \"shares\" and \"sharesOwnedFollowingTransaction\"\n",
    "df = df.drop(columns=[\"shares\", \"sharesOwnedFollowingTransaction\"])\n",
    "\n",
    "issuer_df = df[['issuerCik', 'issuerTicker']]\n",
    "issuer_df['key'] = range(len(issuer_df))\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We break up the relationship column into unique features for each relationship type. For example, if the relationship column contains \"Director\", \"Officer\", and \"Other\", we will create 3 new columns: \"isDirector\", \"isOfficer\", and \"isOther\". (Takes several minutes to get data out of relationship)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_dict(row):\n",
    "    return ast.literal_eval(row)\n",
    "\n",
    "if not os.path.exists('final_df.csv'):\n",
    "        print(\"final_df.csv doesn't exist yet. Creating it (will take more than 10 minutes ...\")\n",
    "        \n",
    "        df['relationship_dict'] = df['relationship'].apply(str_to_dict)\n",
    "        relationship_df = df['relationship_dict'].apply(pd.Series)\n",
    "\n",
    "        print(\"Merging new columns into the original DataFrame...\")\n",
    "        df = pd.concat([df, relationship_df], axis=1)\n",
    "\n",
    "        print(\"Dropping the original 'relationship' column...\")\n",
    "        df = df.drop(['relationship', 'relationship_dict'], axis=1)\n",
    "        \n",
    "        df.to_csv(\"final_df.csv\", index=False)\n",
    "else:\n",
    "    df = pd.read_csv(\"final_df.csv\")\n",
    "    print(\"final_df.csv already exists. Skipping creation...\")\n",
    "\n",
    "df['key'] = range(len(df))\n",
    "df = pd.merge(df, issuer_df[['key', 'issuerCik', 'issuerTicker']], on='key', how='left')\n",
    "df.drop(columns=['key'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the change in value from day 0 (date where the insider trade happened)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform for every sharePriceX add a sharePriceChangeX\n",
    "df[\"sharePriceChange1\"] = (df[\"sharePrice1\"] - df[\"sharePrice0\"]) / df[\"sharePrice0\"]\n",
    "df[\"sharePriceChange7\"] = (df[\"sharePrice7\"] - df[\"sharePrice0\"]) / df[\"sharePrice0\"]\n",
    "df[\"sharePriceChange30\"] = (df[\"sharePrice30\"] - df[\"sharePrice0\"]) / df[\"sharePrice0\"]\n",
    "df[\"sharePriceChange90\"] = (df[\"sharePrice90\"] - df[\"sharePrice0\"]) / df[\"sharePrice0\"]\n",
    "df[\"sharePriceChange360\"] = (df[\"sharePrice360\"] - df[\"sharePrice0\"]) / df[\"sharePrice0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we check what unique values the different features have to be able to handle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"type\"].unique())\n",
    "print(df[\"securityTitle\"].unique())\n",
    "print(df[\"codingCode\"].unique())\n",
    "print(df[\"acquiredDisposed\"].unique())\n",
    "print(df[\"positionDelta\"].unique())\n",
    "print(df[\"isDirector\"].unique())\n",
    "print(df[\"isOfficer\"].unique())\n",
    "print(df[\"isTenPercentOwner\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove all non numerical values from \"positionDelta\" and \"total\" and drop \"inf\" values from \"positionDelta\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all non numberical values from \"positionDelta\"\n",
    "df[\"positionDelta\"] = pd.to_numeric(df[\"positionDelta\"], errors=\"coerce\")\n",
    "\n",
    "# Drop \"inf\" values from \"positionDelta\"\n",
    "df = df.dropna(subset=[\"positionDelta\"])\n",
    "\n",
    "# Remove all non numberical values from \"total\"\n",
    "df[\"total\"] = pd.to_numeric(df[\"total\"], errors=\"coerce\")\n",
    "\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We change the \"type\" feature to be binary (either true or false)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in column \"type\" change \"nonDerivative\" to \"True\" and \"derivative\" to \"False\"\n",
    "df[\"type\"] = df[\"type\"].apply(lambda x: True if x == \"nonDerivative\" else False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we break up the column \"acquiredDisposed\" into two features: \"True\" and \"False\". Which indicates whether a row represents a purchase (True) or a sale (False)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in column \"acquiredDisposed\" change \"A\" to \"True\" and \"D\" to \"False\"\n",
    "df[\"acquiredDisposed\"] = df[\"acquiredDisposed\"].apply(lambda x: True if x == \"A\" else False)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop all irrelevant features\n",
    "Now we remove all unnecessary features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"securityTitle\", \"codingCode\", \"underlyingSecurity\", \"sharePrice1\", \"sharePrice7\", \"sharePrice30\", \"sharePrice90\", \"sharePrice360\", \"officerTitle\", \"isOther\", \"otherText\", \"issuerTicker_y\", \"issuerCik_y\", \"issuerCik_x\", \"issuerTicker_x\", \"sharePrice\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final data adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all rows where total is 0 or NaN\n",
    "df = df[df[\"total\"] != 0]\n",
    "\n",
    "# Drop all rows where positionDelta is 0 or NaN\n",
    "df = df[df[\"positionDelta\"] != 0]\n",
    "\n",
    "# Drop all rows where sharePriceChange7, sharePriceChange30, sharePriceChange90, sharePriceChange360 is NaN or 0\n",
    "df = df[(df[\"sharePriceChange7\"] != 0) & (df[\"sharePriceChange30\"] != 0) & (df[\"sharePriceChange90\"] != 0) & (df[\"sharePriceChange360\"] != 0)]\n",
    "\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total rows cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Initial number of rows: {df_len_0} \\n\\n Final number of rows: {len(df)} \\n Percentage of rows removed: {(df_len_0 - len(df)) / df_len_0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most data had to be removed to a lack in data quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7uS2bEyCJhpM"
   },
   "source": [
    "# 3. Explanatory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXiVj15pJhpM"
   },
   "source": [
    "## 3.1 Descriptive statistics\n",
    "Now we start analysing the data. \n",
    "\n",
    "First, we create a function \"millions_formatter()\" that converts normal numbers into a suitable format.\n",
    "\n",
    "Let's start by comparing the total number of securities acquired and disposed per day. To do this, we first create two dataframes. One contains the total number of acquired and the other of disposed securities. Then we merge both dataframes into one. After that we rename the columns to \"acquired\" and \"disposed\". We sort the trades by date and show the first 10 rows of the resulting dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def millions_formatter(x, pos):\n",
    "    return \"$ {:,.0f} M\".format(x*1e-6)\n",
    "\n",
    "acquired_trades = df[df[\"acquiredDisposed\"]==True].groupby([\"periodOfReport\"])[\"total\"].sum()\n",
    "disposed_trades = df[df[\"acquiredDisposed\"]==False].groupby([\"periodOfReport\"])[\"total\"].sum()\n",
    "\n",
    "acquired_disposed_trades = pd.merge(acquired_trades, disposed_trades, on=\"periodOfReport\", how=\"outer\")\n",
    "acquired_disposed_trades.rename(columns={\"total_x\": \"acquired\", \"total_y\": \"disposed\"}, inplace=True)\n",
    "acquired_disposed_trades = acquired_disposed_trades.sort_values(by=[\"periodOfReport\"])\n",
    "df.info()\n",
    "acquired_disposed_trades.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the new dataframe, we now want to plot the total number of securities acquired and disposed per year.\n",
    "\n",
    "First we start to change the index values of the dataframe to datetime format with \"to_datetime()\". Now we group the acquired and disposed securities into the individual years from 2012 to 2022 using \"groupby()\". Then we merge the two resulting dataframes into a new dataframe. Now we plot the values as a bar graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acquired_disposed_trades.index = pd.to_datetime(acquired_disposed_trades.index)\n",
    "    \n",
    "acquired_year = acquired_disposed_trades.groupby(pd.Grouper(freq=\"Y\"))[\"acquired\"].sum()\n",
    "disposed_year = acquired_disposed_trades.groupby(pd.Grouper(freq=\"Y\"))[\"disposed\"].sum()\n",
    "    \n",
    "acquired_disposed_year = pd.merge(acquired_year, disposed_year, on=\"periodOfReport\", how=\"outer\")\n",
    "    \n",
    "ax = acquired_disposed_year.plot.bar(stacked=False, figsize=(15, 7), color=['#1f77b4', '#ff7f0e'])\n",
    "    \n",
    "ax.grid(True)\n",
    "ax.yaxis.set_major_formatter(matplotlib.ticker.FuncFormatter(millions_formatter))\n",
    "ax.set_xticks(range(acquired_disposed_year.index.size))\n",
    "ax.set_xticklabels([ts.strftime('%Y') for idx, ts in enumerate(acquired_disposed_year.index)])\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"Amount $\")\n",
    "ax.set_title(\"Acquired/Disposed per Year\")\n",
    "ax.figure.autofmt_xdate(rotation=0, ha='center')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_oneDay = df[df[\"periodOfReport\"] == \"2022-12-28\"]\n",
    "\n",
    "# Create the new DataFrame with the desired columns\n",
    "df_priceChange = df[[\"periodOfReport\", \"sharePrice0\", \"sharePriceChange1\", \"sharePriceChange7\", \"sharePriceChange30\", \"sharePriceChange90\", \"sharePriceChange360\"]]\n",
    "\n",
    "# Display the first lines of the new DataFrame for verification\n",
    "df_priceChange.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "140rqd0dJhpM"
   },
   "source": [
    "## 3.2 Distribution of variables\n",
    "Now we visualize the trading distribution for different roles within the company: isDirector, isOfficer, and isTenPercentOwner. We group the data by transaction type (bought or sold) for each role, aggregate the total amounts, and plot the results. It is important to note that there might be an overlap between positions since individuals can hold multiple roles simultaneously.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the roles\n",
    "roles = ['isDirector', 'isOfficer', 'isTenPercentOwner']\n",
    "\n",
    "# Set up the plotting environment\n",
    "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(14, 18))\n",
    "\n",
    "for i, role in enumerate(roles):\n",
    "    # Group by role and transaction type, and sum the total\n",
    "    role_totals = df[df[role]].groupby('acquiredDisposed')['total'].sum().fillna(0)\n",
    "    \n",
    "    # Plot the data\n",
    "    bars = axes[i].bar(role_totals.index.map({False: 'Sold', True: 'Bought'}), role_totals.values, color=['#1f77b4', '#ff7f0e'])\n",
    "    axes[i].set_title(f'Total Amount of Stocks Bought or Sold by {role}')\n",
    "    axes[i].set_xlabel('Transaction Type')\n",
    "    axes[i].set_ylabel('Total Amount')\n",
    "    axes[i].yaxis.set_major_formatter(matplotlib.ticker.FuncFormatter(millions_formatter))\n",
    "    # Set legend\n",
    "    axes[i].legend(bars, ['Sold', 'Bought'], loc='upper right')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by creating a copy of the original DataFrame to avoid modifying the original data. Next, we replace any infinite values with NaN to handle these as missing values. We then remove all rows containing any NaN values to clean the dataset.\n",
    "\n",
    "After ensuring that the cleaned DataFrame (df_cleaned) contains data, we calculate the mean returns for each transaction type (bought or sold) by grouping the data based on the acquiredDisposed column. These mean returns are then printed to provide a summary of the data.\n",
    "\n",
    "To make the results more readable, we map the acquiredDisposed values to descriptive labels: ‘Sold’ for False and ‘Bought’ for True. Finally, we plot these mean returns in a bar chart, displaying the average returns for different holding periods (7 days, 30 days, 90 days, and 360 days) by transaction type. This visualization helps to clearly understand the performance of stocks over various time frames based on whether they were bought or sold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a copy of the original DataFrame to avoid modifying it\n",
    "df_copy = df.copy()\n",
    "\n",
    "# Replace Inf values with NaN\n",
    "df_copy.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Remove all rows with any NaN values in the entire DataFrame\n",
    "df_cleaned = df_copy.dropna()\n",
    "\n",
    "# Ensure df_cleaned has data\n",
    "if df_cleaned.empty:\n",
    "    print(\"No data available after removing NaN and Inf values\")\n",
    "else:\n",
    "    print(f\"Data available after cleaning: {len(df_cleaned)} rows\")\n",
    "\n",
    "    # Calculate mean returns for each transaction type (acquiredDisposed)\n",
    "    mean_returns = df_cleaned.groupby('acquiredDisposed')[['sharePriceChange7', 'sharePriceChange30', 'sharePriceChange90', 'sharePriceChange360']].mean()\n",
    "\n",
    "    # Print the average returns\n",
    "    print(\"Average Returns by Transaction Type:\")\n",
    "    print(mean_returns)\n",
    "\n",
    "    # Map acquiredDisposed values to descriptive labels\n",
    "    mean_returns.index = mean_returns.index.map({False: 'Sold', True: 'Bought'})\n",
    "\n",
    "    # Plot the mean returns\n",
    "    mean_returns.T.plot(kind='bar', figsize=(14, 8))\n",
    "    plt.title('Average Returns by Transaction Type')\n",
    "    plt.xlabel('Return Period')\n",
    "    plt.ylabel('Average Return')\n",
    "    plt.legend(title='Transaction Type')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the average returns following an insider purchase are higher than those following an insider sale across all holding periods. This suggests that there could be a correlation between insider buying or selling activity and subsequent stock price movements. In the following analysis, we will employ machine learning techniques to further investigate and quantify this potential relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpnBBDYEJhpM"
   },
   "source": [
    "## 3.3 Normal Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Separate dataframes based on acquisition type\n",
    "acquisitions_df = df[df['acquiredDisposed'] == True]\n",
    "dispositions_df = df[df['acquiredDisposed'] == False]\n",
    "\n",
    "# Select features for analysis (excluding target variables and binary flags)\n",
    "features = ['total', 'sharePrice0', 'positionDelta']\n",
    "target_vars = ['sharePriceChange7', 'sharePriceChange30', 'sharePriceChange90', 'sharePriceChange360']\n",
    "\n",
    "# Function to create histogram data\n",
    "def get_hist_data(data, bins=100):\n",
    "    # Remove infinite values and convert to float64\n",
    "    data_clean = data[np.isfinite(data)].astype(np.float64)\n",
    "    if len(data_clean) == 0:\n",
    "        return None, None  # Return None if all data is invalid\n",
    "    hist, bin_edges = np.histogram(data_clean, bins=bins)\n",
    "    return bin_edges, hist\n",
    "\n",
    "# Function to safely compute mean\n",
    "def safe_mean(data):\n",
    "    return np.mean(data[np.isfinite(data)])\n",
    "\n",
    "# Function to safely check if we should use log scale\n",
    "def should_use_log_scale(data):\n",
    "    non_zero = data[(data != 0) & np.isfinite(data)]\n",
    "    if len(non_zero) == 0:\n",
    "        return False\n",
    "    return non_zero.max() / non_zero.min() > 100\n",
    "\n",
    "def plot_distributions(features, df_a, df_b, name_a, name_b, max_samples):\n",
    "    fig, axs = plt.subplots(len(features), 1, figsize=(15, 5 * len(features)))\n",
    "    axs = axs.ravel()\n",
    "\n",
    "    for i, feature in enumerate(features):\n",
    "        a = df_a[feature]\n",
    "        b = df_b[feature]\n",
    "        \n",
    "        # Reduce sample size if necessary\n",
    "        if len(a) > max_samples:\n",
    "            a = a.sample(n=max_samples, random_state=42)\n",
    "        if len(b) > max_samples:\n",
    "            b = b.sample(n=max_samples, random_state=42)\n",
    "        \n",
    "        # Ensure equal sample sizes for t-test\n",
    "        min_samples = min(len(a), len(b))\n",
    "        a = a.sample(n=min_samples, random_state=42)\n",
    "        b = b.sample(n=min_samples, random_state=42)\n",
    "        \n",
    "        # Remove non-finite values for t-test\n",
    "        a_clean = a[np.isfinite(a)]\n",
    "        b_clean = b[np.isfinite(b)]\n",
    "        \n",
    "        # Running t-tests only if we have valid data\n",
    "        if len(a_clean) > 0 and len(b_clean) > 0:\n",
    "            t_stat, p_value = stats.ttest_ind(a_clean, b_clean)\n",
    "        else:\n",
    "            t_stat, p_value = np.nan, np.nan\n",
    "        \n",
    "        # Create distribution plots using matplotlib's hist\n",
    "        a_bins, a_hist = get_hist_data(a)\n",
    "        b_bins, b_hist = get_hist_data(b)\n",
    "        \n",
    "        if a_bins is not None and b_bins is not None:\n",
    "            axs[i].hist(a, bins=a_bins, weights=np.ones_like(a)/len(a), alpha=0.5, label=name_a, color='blue')\n",
    "            axs[i].hist(b, bins=b_bins, weights=np.ones_like(b)/len(b), alpha=0.5, label=name_b, color='red')\n",
    "            \n",
    "            axs[i].set_title(f\"{feature} (t-stat: {t_stat:.2f}, p-value: {p_value:.4f})\")\n",
    "            axs[i].legend()\n",
    "            \n",
    "            # Add vertical lines for means\n",
    "            a_mean = safe_mean(a)\n",
    "            b_mean = safe_mean(b)\n",
    "            axs[i].axvline(a_mean, color='blue', linestyle='dashed', linewidth=2)\n",
    "            axs[i].axvline(b_mean, color='red', linestyle='dashed', linewidth=2)\n",
    "            \n",
    "            # Set x-axis to log scale if the feature has a wide range\n",
    "            if should_use_log_scale(a) or should_use_log_scale(b):\n",
    "                axs[i].set_xscale('log')\n",
    "                axs[i].set_xlim(left=max(a.min(), b.min()) / 10)  # Set a non-zero lower limit\n",
    "        else:\n",
    "            axs[i].text(0.5, 0.5, \"Insufficient valid data for plotting\", \n",
    "                        ha='center', va='center', transform=axs[i].transAxes)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Set maximum number of samples to use (adjust based on your memory constraints)\n",
    "max_samples = 100000\n",
    "\n",
    "# Plot distributions for features\n",
    "plot_distributions(features, acquisitions_df, dispositions_df, \"Acquisitions\", \"Dispositions\", max_samples)\n",
    "\n",
    "# Plot distributions for target variables\n",
    "plot_distributions(target_vars, acquisitions_df, dispositions_df, \"Acquisitions\", \"Dispositions\", max_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3.4 Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant features for correlation analysis\n",
    "features = ['total', 'sharePrice0', 'positionDelta', 'isDirector', 'isOfficer', 'isTenPercentOwner',\n",
    "            'sharePriceChange7', 'sharePriceChange30', 'sharePriceChange90', 'sharePriceChange360']\n",
    "\n",
    "# Create correlation matrix\n",
    "corr_matrix = df[features].corr()\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(corr_matrix, \n",
    "            vmin=-1, \n",
    "            vmax=1, \n",
    "            cmap=sns.diverging_palette(20, 220, as_cmap=True),\n",
    "            annot=True,\n",
    "            fmt='.2f',\n",
    "            center=0)\n",
    "\n",
    "plt.title('Correlation Heatmap of Insider Trading Features', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3.5 Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant numerical features for boxplots\n",
    "features = ['total', 'sharePrice0', 'positionDelta', \n",
    "            'sharePriceChange7', 'sharePriceChange30', 'sharePriceChange90', 'sharePriceChange360']\n",
    "\n",
    "# Melt the dataframe to long format for easier plotting\n",
    "df_melt = pd.melt(df[features], var_name='Feature', value_name='Value')\n",
    "\n",
    "# Create the boxplots\n",
    "fig, ax = plt.subplots(1, figsize=(17, 6))\n",
    "sns.boxplot(data=df_melt, x='Feature', y='Value', ax=ax)\n",
    "\n",
    "# Rotate x-axis labels\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Set y-axis to log scale\n",
    "ax.set_yscale(\"log\")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Distribution of Insider Trading Features', fontsize=16)\n",
    "plt.xlabel('Features', fontsize=12)\n",
    "plt.ylabel('Value (log scale)', fontsize=12)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Additional boxplots for binary features\n",
    "binary_features = ['isDirector', 'isOfficer', 'isTenPercentOwner', 'acquiredDisposed']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(binary_features):\n",
    "    sns.boxplot(data=df, x=feature, y='total', ax=axes[i])\n",
    "    axes[i].set_yscale(\"log\")\n",
    "    axes[i].set_title(f'Total Transaction Amount by {feature}', fontsize=14)\n",
    "    axes[i].set_xlabel(feature, fontsize=12)\n",
    "    axes[i].set_ylabel('Total Amount (log scale)', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Prb79LK-JhpN"
   },
   "source": [
    "# 4. Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wO6FZRH1JhpN"
   },
   "source": [
    "## 4.1 Data Splitting\n",
    "We define a function clean_data to clean and preprocess the data. In this function, we replace infinity values with NaN, remove rows with NaN values, and cap extreme values at the 1st and 99th percentiles. Finally, we clean the data by applying the clean_data function to the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to clean and preprocess data\n",
    "def clean_data(df):\n",
    "    # Replace infinity with NaN\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Remove rows with NaN values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Cap extreme values (e.g., 99th percentile)\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        lower = df[col].quantile(0.01)\n",
    "        upper = df[col].quantile(0.99)\n",
    "        df[col] = df[col].clip(lower, upper)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Clean the data\n",
    "df = clean_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select specific columns from the DataFrame df for our feature matrix X and target matrix y. The selected features for X include ‘total’, ‘sharePrice0’, ‘positionDelta’, ‘isDirector’, ‘isOfficer’, ‘isTenPercentOwner’, and ‘type’. The target variables y include ‘sharePriceChange7’, ‘sharePriceChange30’, ‘sharePriceChange90’, and ‘sharePriceChange360’. Additionally, we create a new feature isAcquisition in X to indicate whether the transaction was an acquisition. Finally, we split the data into training and testing sets using an 80-20 split, with a random seed of 42 for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93TK6rwxJhpN"
   },
   "outputs": [],
   "source": [
    "X = df[[ 'total', 'sharePrice0', 'positionDelta', 'isDirector', 'isOfficer', 'isTenPercentOwner', \"type\"]]\n",
    "y = df[['sharePriceChange7', 'sharePriceChange30', 'sharePriceChange90', 'sharePriceChange360']]\n",
    "\n",
    "X['isAcquisition'] = (df['acquiredDisposed'] == True)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Regression\n",
    "We define a set of models along with their hyperparameter grids for training. These models include Linear Regression, Ridge Regression, Lasso Regression, Random Forest Regressor, XGBoost Regressor, LightGBM Regressor, and CatBoost Regressor. Each model is accompanied by a specific set of hyperparameters to be tuned using grid search.\n",
    "\n",
    "Next, we create a function train_and_evaluate to handle the training and evaluation of these models. This function performs grid search with cross-validation to identify the optimal hyperparameters, fits the model on the training data, and evaluates it on the test data using metrics like mean squared error (MSE) and R-squared (R2). The function returns both the results and the best-fitted models.\n",
    "\n",
    "To display the results and feature importances, we define another function print_results_and_importances. This function prints out the best parameters, cross-validated R2 score, test MSE, and test R2 for each model. For tree-based models, it also shows the feature importances.\n",
    "\n",
    "We then proceed to train and evaluate the models for each target variable (sharePriceChange7, sharePriceChange30, sharePriceChange90, sharePriceChange360) separately for acquisition and disposition transactions. The data is split into acquisition and disposition subsets, and models are trained on these subsets. The results and feature importances are then printed.\n",
    "\n",
    "Finally, predictions are made using the best models for each target variable and acquisition type. We print a sample of these predictions to illustrate how well the models perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ltmcOVclJhpN"
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define models with hyperparameter grids\n",
    "# models = {\n",
    "#     'Linear Regression': (LinearRegression(), {}),\n",
    "#     'Ridge Regression': (Ridge(), {'alpha': np.logspace(-3, 3, 5)}),\n",
    "#     'Lasso Regression': (Lasso(), {'alpha': np.logspace(-3, 3, 5)}),\n",
    "#     'Random Forest Regressor': (RandomForestRegressor(), {\n",
    "#         'n_estimators': [100, 200],\n",
    "#         'max_depth': [10, 20],\n",
    "#         'min_samples_split': [2, 5]\n",
    "#     }),\n",
    "#     'XGBoost Regressor': (xgb.XGBRegressor(), {\n",
    "#         'n_estimators': [100, 200],\n",
    "#         'learning_rate': [0.01, 0.1],\n",
    "#         'max_depth': [3, 5]\n",
    "#     }),\n",
    "#     'LightGBM Regressor': (lgb.LGBMRegressor(verbose=-1), {\n",
    "#         'n_estimators': [100, 200],\n",
    "#         'learning_rate': [0.01, 0.1],\n",
    "#         'num_leaves': [31, 62]\n",
    "#     }),\n",
    "#     'CatBoost Regressor': (cb.CatBoostRegressor(verbose=0), {\n",
    "#         'iterations': [100, 200],\n",
    "#         'learning_rate': [0.01, 0.1],\n",
    "#         'depth': [4, 6]\n",
    "#     })\n",
    "# }\n",
    "\n",
    "# def train_with_timeout(model, X_train, y_train, timeout=300):\n",
    "#     with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "#         future = executor.submit(model.fit, X_train, y_train)\n",
    "#         try:\n",
    "#             return future.result(timeout=timeout)\n",
    "#         except TimeoutError:\n",
    "#             print(f\"Training timed out after {timeout} seconds\")\n",
    "#             return None\n",
    "\n",
    "# def train_and_evaluate(X_train, X_test, y_train, y_test):\n",
    "#     results = {}\n",
    "#     fitted_models = {}\n",
    "#     for name, (model, param_grid) in models.items():\n",
    "#         print(f\"Training {name}\")\n",
    "#         try:\n",
    "#             random_search = RandomizedSearchCV(model, param_grid, n_iter=3, cv=3, scoring='r2', n_jobs=-1, random_state=42)\n",
    "#             best_model = train_with_timeout(random_search, X_train, y_train)\n",
    "            \n",
    "#             if best_model is None:\n",
    "#                 raise Exception(\"Model training timed out\")\n",
    "            \n",
    "#             y_pred = best_model.predict(X_test)\n",
    "            \n",
    "#             mse = mean_squared_error(y_test, y_pred)\n",
    "#             mae = mean_absolute_error(y_test, y_pred)\n",
    "#             r2 = r2_score(y_test, y_pred)\n",
    "            \n",
    "#             results[name] = {\n",
    "#                 'Best Parameters': random_search.best_params_,\n",
    "#                 'CV R2': random_search.best_score_,\n",
    "#                 'Test MSE': mse,\n",
    "#                 'Test MAE': mae,\n",
    "#                 'Test R2': r2,\n",
    "#                 'Predictions': y_pred\n",
    "#             }\n",
    "#             fitted_models[name] = best_model\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error in {name}: {str(e)}\")\n",
    "#             results[name] = {\n",
    "#                 'Best Parameters': None,\n",
    "#                 'CV R2': np.nan,\n",
    "#                 'Test MSE': np.nan,\n",
    "#                 'Test MAE': np.nan,\n",
    "#                 'Test R2': -np.inf,\n",
    "#                 'Predictions': None\n",
    "#             }\n",
    "#     return results, fitted_models\n",
    "\n",
    "# def plot_feature_importances(best_model, X, target):\n",
    "#     if hasattr(best_model, 'feature_importances_'):\n",
    "#         feature_importance = best_model.feature_importances_\n",
    "#         feature_names = X.columns\n",
    "#         importance_df = pd.DataFrame({'feature': feature_names, 'importance': feature_importance})\n",
    "#         importance_df = importance_df.sort_values('importance', ascending=True)\n",
    "        \n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         plt.barh(importance_df['feature'], importance_df['importance'])\n",
    "#         plt.xlabel('Importance')\n",
    "#         plt.ylabel('Feature')\n",
    "#         plt.title(f'Feature Importances for {target}')\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "\n",
    "# def plot_model_performance(y_test, predictions, model_name, target):\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.scatter(y_test, predictions, alpha=0.5)\n",
    "#     plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "#     plt.xlabel('Actual Values')\n",
    "#     plt.ylabel('Predicted Values')\n",
    "#     plt.title(f'{model_name} - {target}')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# def plot_residuals(y_test, predictions, model_name, target):\n",
    "#     residuals = y_test - predictions\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     sns.scatterplot(x=predictions, y=residuals)\n",
    "#     plt.axhline(y=0, color='r', linestyle='--')\n",
    "#     plt.xlabel('Predicted Values')\n",
    "#     plt.ylabel('Residuals')\n",
    "#     plt.title(f'Residual Plot for {model_name} - {target}')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# def plot_model_comparison(results, target):\n",
    "#     model_names = list(results.keys())\n",
    "#     r2_scores = [results[model]['Test R2'] for model in model_names]\n",
    "#     mse_scores = [results[model]['Test MSE'] for model in model_names]\n",
    "#     mae_scores = [results[model]['Test MAE'] for model in model_names]\n",
    "\n",
    "#     fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 18))\n",
    "\n",
    "#     ax1.bar(model_names, r2_scores)\n",
    "#     ax1.set_ylabel('R2 Score')\n",
    "#     ax1.set_title(f'Model Comparison - R2 Score for {target}')\n",
    "#     ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "#     ax2.bar(model_names, mse_scores)\n",
    "#     ax2.set_ylabel('Mean Squared Error')\n",
    "#     ax2.set_title(f'Model Comparison - MSE for {target}')\n",
    "#     ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "#     ax3.bar(model_names, mae_scores)\n",
    "#     ax3.set_ylabel('Mean Absolute Error')\n",
    "#     ax3.set_title(f'Model Comparison - MAE for {target}')\n",
    "#     ax3.tick_params(axis='x', rotation=45)\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# def main(df):\n",
    "#     X = df[['total', 'sharePrice0', 'positionDelta', 'isDirector', 'isOfficer', 'isTenPercentOwner', 'type']]\n",
    "#     y = df[['sharePriceChange7', 'sharePriceChange30', 'sharePriceChange90', 'sharePriceChange360']]\n",
    "#     X['isAcquisition'] = (df['acquiredDisposed'] == True)\n",
    "\n",
    "#     # Reduce dataset size for testing\n",
    "#     X = X.sample(n=1000, random_state=42)\n",
    "#     y = y.loc[X.index]\n",
    "\n",
    "#     all_results = {}\n",
    "#     best_models = {}\n",
    "\n",
    "#     for acquisition_type in ['Acquisitions', 'Dispositions']:\n",
    "#         print(f\"\\nTraining models for {acquisition_type}:\")\n",
    "        \n",
    "#         mask = X['isAcquisition'] if acquisition_type == 'Acquisitions' else ~X['isAcquisition']\n",
    "#         X_filtered = X[mask].drop('isAcquisition', axis=1)\n",
    "#         y_filtered = y[mask]\n",
    "        \n",
    "#         for target in y.columns:\n",
    "#             print(f\"\\nTarget: {target}\")\n",
    "            \n",
    "#             X_train, X_test, y_train, y_test = train_test_split(X_filtered, y_filtered[target], test_size=0.2, random_state=42)\n",
    "#             results, fitted_models = train_and_evaluate(X_train, X_test, y_train, y_test)\n",
    "            \n",
    "#             # Find the best model that actually fitted\n",
    "#             best_model_name = max((name for name in results if results[name]['Test R2'] != -np.inf), \n",
    "#                                   key=lambda x: results[x]['Test R2'],\n",
    "#                                   default=None)\n",
    "            \n",
    "#             if best_model_name is not None and best_model_name in fitted_models:\n",
    "#                 best_model = fitted_models[best_model_name]\n",
    "#                 print(f\"Best model: {best_model_name}\")\n",
    "                \n",
    "#                 plot_feature_importances(best_model, X_filtered, target)\n",
    "#                 plot_model_performance(y_test, results[best_model_name]['Predictions'], best_model_name, target)\n",
    "#                 plot_residuals(y_test, results[best_model_name]['Predictions'], best_model_name, target)\n",
    "#             else:\n",
    "#                 print(\"No models were successfully fitted.\")\n",
    "            \n",
    "#             plot_model_comparison(results, target)\n",
    "            \n",
    "#             if target not in all_results:\n",
    "#                 all_results[target] = {}\n",
    "#             all_results[target][acquisition_type] = results\n",
    "            \n",
    "#             if target not in best_models:\n",
    "#                 best_models[target] = {}\n",
    "#             best_models[target][acquisition_type] = best_model if best_model_name is not None else None\n",
    "\n",
    "#     return all_results, best_models\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     all_results, best_models = main(df)\n",
    "#     print(\"Analysis completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0NmFfvEJhpO"
   },
   "source": [
    "## 4.3 Classification Models\n",
    "\n",
    "Now we set up a process for training and evaluating classification models to predict share price changes based on various features. Initially, it configures logging to track the entire process.\r\n",
    "\r\n",
    "The first step involves data preparation, where feature columns (X) and target columns (y) are extracted from a DataFrame (df). The target columns, which represent share price changes over different periods, are converted to binary classes, indicating whether the share price change is positive or not. Additionally, an extra feature is added to X to indicate whether a transaction is an acquisition.\r\n",
    "\r\n",
    "Next we define three classification models (Random Forest, XGBoost, and LightGBM) along with a reduced set of hyperparameters for each. These models will be evaluated to determine the best one for predicting share price changes.\r\n",
    "\r\n",
    "Then we define a function to handle the training and evaluation of these models. This function uses RandomizedSearchCV for hyperparameter tuning and evaluates the models based on metrics such as accuracy, precision, recall, F1 score, ROC AUC, and the confusion matrix. The results and feature importances are logged for further analysis.\r\n",
    "\r\n",
    "Additionally, we create a function to plot the ROC curve for the models, providing a visual representation of their performance.\r\n",
    "\r\n",
    "Then we split the data into acquisitions and dispositions and trains and evaluates the models separately for each category and each target variable (representing different periods of share price change). The results are logged and plotted, offering a comprehensive overview of the models' performance.\r\n",
    "\r\n",
    "Finally, the best models are used to make predictions on the test data. A sample of these predictions is logged, showcasing the models' ability to predict share price changes. The process concludes with the logging of final predictions, ensuring a thorough evaluation and understanding of the models' performance throughout the entire pipeline.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y1Yysa8VJhpO"
   },
   "outputs": [],
   "source": [
    " Prepare data\n",
    "X = df[['total', 'sharePrice0', 'positionDelta', 'isDirector', 'isOfficer', 'isTenPercentOwner', \"type\"]]\n",
    "y = df['sharePriceChange360'] > 0  # Convert to binary outcome\n",
    "\n",
    "# Filter for acquisitions only\n",
    "acquisitions_mask = df['acquiredDisposed'] == True\n",
    "X = X[acquisitions_mask]\n",
    "y = y[acquisitions_mask]\n",
    "\n",
    "# Define models with reduced hyperparameter grids\n",
    "models = {\n",
    "    'Random Forest': (RandomForestClassifier(random_state=42), {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 20],\n",
    "        'min_samples_split': [2, 5]\n",
    "    }),\n",
    "    'XGBoost': (xgb.XGBClassifier(random_state=42), {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'max_depth': [3, 5]\n",
    "    }),\n",
    "    'LightGBM': (lgb.LGBMClassifier(random_state=42, verbose=-1), {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'num_leaves': [31, 63]\n",
    "    })\n",
    "}\n",
    "\n",
    "def train_and_evaluate(model, param_grid, X_train, X_test, y_train, y_test):\n",
    "    random_search = RandomizedSearchCV(model, param_grid, n_iter=5, cv=3, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = random_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    return {\n",
    "        'Best Parameters': random_search.best_params_,\n",
    "        'CV Accuracy': random_search.best_score_,\n",
    "        'Test Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Test Precision': precision_score(y_test, y_pred, average='weighted'),\n",
    "        'Test Recall': recall_score(y_test, y_pred, average='weighted'),\n",
    "        'Test F1': f1_score(y_test, y_pred, average='weighted'),\n",
    "        'ROC AUC': auc(roc_curve(y_test, y_pred_proba)[0], roc_curve(y_test, y_pred_proba)[1]),\n",
    "        'Confusion Matrix': confusion_matrix(y_test, y_pred),\n",
    "        'Classification Report': classification_report(y_test, y_pred),\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'Model': best_model\n",
    "    }\n",
    "\n",
    "def plot_results(results):\n",
    "    # Feature Importance\n",
    "    best_model = max(results.items(), key=lambda x: x[1]['Test Accuracy'])[1]['Model']\n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        importance = best_model.feature_importances_\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.barh(X.columns, importance)\n",
    "        plt.title(\"Feature Importance - Acquisitions\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # ROC Curve\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for name, metrics in results.items():\n",
    "        fpr, tpr, _ = roc_curve(y_test, metrics['y_pred_proba'])\n",
    "        plt.plot(fpr, tpr, label=f'{name} (AUC = {metrics[\"ROC AUC\"]:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve - Acquisitions')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = results[max(results, key=lambda x: results[x]['Test Accuracy'])]['Confusion Matrix']\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix - Acquisitions')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "\n",
    "    # Metric Comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    metrics = [results[model]['Test Accuracy'] for model in results.keys()]\n",
    "    plt.bar(results.keys(), metrics)\n",
    "    plt.title('Model Comparison - Acquisitions')\n",
    "    plt.ylabel('Test Accuracy')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "for name, (model, param_grid) in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    results[name] = train_and_evaluate(model, param_grid, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Plot results\n",
    "plot_results(results)\n",
    "\n",
    "# Print best model results\n",
    "print(\"\\nBest model results:\")\n",
    "best_model_name = max(results, key=lambda x: results[x]['Test Accuracy'])\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "print(f\"Test Accuracy: {results[best_model_name]['Test Accuracy']:.4f}\")\n",
    "print(f\"ROC AUC: {results[best_model_name]['ROC AUC']:.4f}\")\n",
    "print(f\"\\nClassification Report:\\n{results[best_model_name]['Classification Report']}\")\n",
    "\n",
    "print(\"\\nProcess completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwezNQDdJhpO"
   },
   "source": [
    "## 4.4 Limitations\n",
    "Analysing insider data is associated with a number of restrictions. The most significant limitation is the limited time horizon in which the SEC API analyses insider data (since 2012). This corresponds to a total time horizon of approximately 10 years, which is not necessarily representative of the entire period of the stock market. Furthermore, some entries had to be removed due to incorrect documentation in the SEC API, which could further dilute the results of the analysis. Consequently, the results of our analysis provide only a limited insight into the nature of insider trading, particularly in the context of the period from 2012 to 2022, which was characterised by a strong bull market."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "br6pdCqBJhpO"
   },
   "source": [
    "## 4.5 Key Findings\n",
    "The analysis of insider trading data using various regression and classification models revealed significant insights. The Random Forest model emerged as the best performing model, achieving a test accuracy of 70.61% and a ROC AUC score of 0.7802. The classification report indicates that the model has a high recall for predicting true acquisition outcomes (95%), but the precision is relatively moderate (68%). This suggests that while the model is highly effective at identifying true positive cases of acquisitions, it has a considerable number of false positives. The precision-recall balance in predicting false outcomes shows a much lower recall (35%) but higher precision (84%). Overall, the model performs well in distinguishing between acquisition and non-acquisition events, making it a reliable tool for predicting insider trading activities based on the provided features.\n",
    "\n",
    "The confusion matrix for the Random Forest model further illustrates the model’s performance, showing that the model correctly predicted 48,177 true acquisition cases and 12,158 non-acquisition cases. However, there were 22,813 false positives and 2,296 false negatives. The ROC curve comparison indicates that the Random Forest model outperforms both the XGBoost and LightGBM models, with the highest AUC value of 0.78.\n",
    "\n",
    "The feature importance analysis reveals that ‘sharePrice0’, ‘total’, and ‘positionDelta’ are the most significant predictors in the model. These features contribute substantially to the model’s ability to predict insider trading activities. The insights gained from this analysis can aid regulatory bodies and investors by providing a predictive mechanism for insider trading behaviors, enhancing decision-making processes and potentially identifying suspicious trading activities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Further Research\n",
    "The results from the insider trading data analysis can be used to form a robust investment strategy by leveraging the insights gained from corporate insiders’ trading behaviors. This strategy is predicated on the belief that insiders, such as directors, executives, and major shareholders, have unique knowledge about their company’s financial health and future prospects. By systematically analyzing their trading activities, you can make informed investment decisions, potentially generating substantial returns and minimizing risks.\n",
    "\n",
    "The first step is to gather and prepare insider trading data from reliable sources like SEC filings, financial news platforms, and specialized financial data providers. Cleaning the data is crucial; this involves removing erroneous or incomplete entries and addressing extreme outliers to ensure the dataset is accurate and comprehensive. Key features to focus on include trade volume, share price at the time of the trade, the insider’s position (director, officer, major shareholder), and the percentage of shares traded relative to their total holdings.\n",
    "\n",
    "Next, utilize advanced analytical models to predict the success of insider trades and evaluate their impact on stock prices. Implementing a Random Forest model can help identify the most critical predictors of successful trades based on these features. Conducting a feature importance analysis will further highlight which factors most significantly influence the success of insider trades, such as ‘sharePrice0’, ‘total’, and ‘positionDelta’.\n",
    "\n",
    "The core of the strategy involves specific trading rules derived from the analysis. For buy signals, focus on purchasing stocks when insiders make significant buy transactions, especially when the Random Forest model predicts a high probability of success. Pay particular attention to trades where insiders hold substantial stakes, as this indicates strong confidence in the company’s future. For sell signals, avoid or divest stocks when insiders make substantial sell transactions, particularly if the Random Forest model indicates a potential downturn. Be vigilant about scenarios where multiple insiders are selling shares simultaneously, as this may signal internal concerns about the company’s future performance.\n",
    "\n",
    "Continuous monitoring and adjustments are essential. Regularly review the performance of your investments and adjust the strategy as needed based on ongoing analysis. Reassess the models and data periodically to ensure they remain accurate and relevant in changing market conditions.\n",
    "\n",
    "Further research can enhance this strategy. Continuously refine the predictive models by integrating more advanced machine learning techniques and updating them with the latest data. Exploring deep learning models could capture more complex patterns in insider trading behavior. Additionally, incorporating sentiment analysis of news articles, earnings calls, and social media can complement insider trading data, providing a more comprehensive view of market sentiment and potential stock movements. Investigating alternative data sources, such as credit card transactions, satellite imagery, and web traffic data, can offer additional insights into a company’s performance and future prospects. Developing sophisticated risk management frameworks is necessary to protect against market volatility and unforeseen events. This includes setting stop-loss orders and diversifying the investment portfolio to mitigate risks. Staying informed about changes in regulations regarding insider trading and corporate disclosures will help you adjust the strategy accordingly, ensuring compliance and capitalizing on new opportunities arising from regulatory shifts.\n",
    "\n",
    "By combining insider trading analysis with these advanced techniques and ongoing research, you can develop a robust and adaptive investment strategy that leverages the unique insights provided by corporate insiders, enhancing decision-making processes and potentially identifying lucrative investment opportunities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
